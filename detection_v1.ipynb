{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T19:18:33.511374Z",
     "start_time": "2024-04-12T19:18:33.501908Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab8d2dd16ea3c7f5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T19:18:54.993452Z",
     "start_time": "2024-04-12T19:18:33.927325Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Directory where CSV files are stored\n",
    "csv_directory = '/Users/aimee/Documents/College/Courses/S24/17-735/17735-project/ExtractedData'\n",
    "\n",
    "# Lists to hold data from each file\n",
    "logon_data = []\n",
    "logoff_data = []\n",
    "\n",
    "# Iterate over each CSV file in the directory\n",
    "for filename in os.listdir(csv_directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Full path to the CSV file\n",
    "        csv_path = os.path.join(csv_directory, filename)\n",
    "\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(csv_path, names=['User', 'Timestamp', 'PC', 'ActivityType', 'Action'])\n",
    "\n",
    "        # Convert Timestamp to datetime\n",
    "        df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "        # Extract time and hour from Timestamp\n",
    "        df['Time'] = df['Timestamp'].dt.time\n",
    "        df['Hour'] = df['Timestamp'].dt.hour\n",
    "\n",
    "        # Append DataFrame to the list by action\n",
    "        logon_data.append(df[df['Action'] == 'Logon'])\n",
    "        logoff_data.append(df[df['Action'] == 'Logoff'])\n",
    "\n",
    "# Concatenate all logon and logoff data into two separate DataFrames\n",
    "logon_df = pd.concat(logon_data)\n",
    "logoff_df = pd.concat(logoff_data)\n",
    "\n",
    "# Split the data so that 80% is used for model training, 20% for testing\n",
    "logon_train_df, logon_test_df = train_test_split(logon_df, test_size=0.2)\n",
    "logoff_train_df, logoff_test_df = train_test_split(logoff_df, test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7e19839",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training data\n",
    "\n",
    "# Group by 'User' and aggregate times for logon\n",
    "df_user_logon_stats = logon_train_df.groupby('User')['Time'].agg(['min', 'max']).reset_index()\n",
    "df_logon_mode = logon_train_df.groupby('User')['Time'].agg(lambda x: x.value_counts().index[0]).reset_index()\n",
    "df_logon_mean = logon_train_df.groupby('User')['Hour'].mean().reset_index()\n",
    "\n",
    "# Convert mean hour to int and then to time\n",
    "df_logon_mean['Hour'] = df_logon_mean['Hour'].astype(int)\n",
    "df_logon_mean['Hour'] = pd.to_datetime(df_logon_mean['Hour'], format='%H').dt.time\n",
    "\n",
    "# Add mode and mean to the logon stats DataFrame\n",
    "df_user_logon_stats['mode'] = df_logon_mode['Time']\n",
    "df_user_logon_stats['mean'] = df_logon_mean['Hour']\n",
    "\n",
    "# Group by 'User' and aggregate times for logoff\n",
    "df_user_logoff_stats = logoff_train_df.groupby('User')['Time'].agg(['min', 'max']).reset_index()\n",
    "df_logoff_mode = logoff_train_df.groupby('User')['Time'].agg(lambda x: x.value_counts().index[0]).reset_index()\n",
    "df_logoff_mean = logoff_train_df.groupby('User')['Hour'].mean().reset_index()\n",
    "\n",
    "# Convert mean hour to int and then to time\n",
    "df_logoff_mean['Hour'] = df_logoff_mean['Hour'].astype(int)\n",
    "df_logoff_mean['Hour'] = pd.to_datetime(df_logoff_mean['Hour'], format='%H').dt.time\n",
    "\n",
    "# Add mode and mean to the logoff stats DataFrame\n",
    "df_user_logoff_stats['mode'] = df_logoff_mode['Time']\n",
    "df_user_logoff_stats['mean'] = df_logoff_mean['Hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bc539cae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27301 49617 27629 ... 49617 27629 32400]\n",
      " [28203 29393 28554 ... 29393 28554 25200]\n",
      " [28203 29398 28816 ... 29398 28816 25200]\n",
      " ...\n",
      " [23700 59713 24716 ... 59713 24716 28800]\n",
      " [30005 31186 30335 ... 31186 30335 28800]\n",
      " [32706 33894 33348 ... 33894 33348 32400]]\n",
      "[0.15016086 0.18741806 0.18519014 0.15753546 0.15926661 0.15383357\n",
      " 0.17178882 0.15934769 0.14871345 0.17421431]\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "def dtt2timestamp(dtt):\n",
    "  time_in_sec = (dtt.hour*60 + dtt.minute) * 60 + dtt.second\n",
    "  return time_in_sec\n",
    "\n",
    "df_user_logon_stats_sec = df_user_logon_stats\n",
    "\n",
    "time_columns = ['min', 'max', 'mode', 'mean']\n",
    "for col in time_columns:\n",
    "    df_user_logon_stats_sec[col] = pd.to_datetime(df_user_logon_stats[col], format='%H:%M:%S').dt.time\n",
    "\n",
    "\n",
    "min_ts = [dtt2timestamp(dtt) for dtt in df_user_logon_stats_sec['min']]\n",
    "max_ts = [dtt2timestamp(dtt) for dtt in df_user_logon_stats_sec['max']]\n",
    "mode_ts = [dtt2timestamp(dtt) for dtt in df_user_logon_stats_sec['mode']]\n",
    "mean_ts = [dtt2timestamp(dtt) for dtt in df_user_logon_stats_sec['mean']]\n",
    "\n",
    "df_user_logon_stats_sec['min_ts'] = min_ts\n",
    "df_user_logon_stats_sec['max_ts'] = max_ts\n",
    "df_user_logon_stats_sec['mode_ts'] = mode_ts\n",
    "df_user_logon_stats_sec['mean_ts'] = mean_ts\n",
    "\n",
    "\n",
    "df_user_logon_stats_sec.drop(['min','max','mode','mean'], axis=1)\n",
    "\n",
    "df_user_logoff_stats_sec = df_user_logoff_stats\n",
    "\n",
    "for col in time_columns:\n",
    "    df_user_logoff_stats_sec[col] = pd.to_datetime(df_user_logoff_stats[col], format='%H:%M:%S').dt.time\n",
    "\n",
    "\n",
    "min_ts = [dtt2timestamp(dtt) for dtt in df_user_logoff_stats_sec['min']] \n",
    "max_ts = [dtt2timestamp(dtt) for dtt in df_user_logoff_stats_sec['max']]\n",
    "mode_ts = [dtt2timestamp(dtt) for dtt in df_user_logoff_stats_sec['mode']]\n",
    "mean_ts = [dtt2timestamp(dtt) for dtt in df_user_logoff_stats_sec['mean']]\n",
    "\n",
    "\n",
    "df_user_logoff_stats_sec['min_ts'] = min_ts\n",
    "df_user_logoff_stats_sec['max_ts'] = max_ts\n",
    "df_user_logoff_stats_sec['mode_ts'] = mode_ts\n",
    "df_user_logoff_stats_sec['mean_ts'] = mean_ts\n",
    "\n",
    "\n",
    "df_user_logoff_stats_sec.drop(['min', 'max','mode','mean'], axis=1)\n",
    "\n",
    "\n",
    "df_log_on_off_stats = pd.DataFrame()\n",
    "\n",
    "df_log_on_off_stats['User'] = df_user_logon_stats_sec['User']\n",
    "df_log_on_off_stats['on_min_ts'] = df_user_logon_stats_sec['min_ts']\n",
    "df_log_on_off_stats['on_max_ts'] = df_user_logon_stats_sec['max_ts']\n",
    "df_log_on_off_stats['on_mode_ts'] = df_user_logon_stats_sec['mode_ts']\n",
    "df_log_on_off_stats['on_mean_ts'] = df_user_logon_stats_sec['mean_ts']\n",
    "df_log_on_off_stats['off_min_ts'] = df_user_logon_stats_sec['min_ts']\n",
    "df_log_on_off_stats['off_max_ts'] = df_user_logon_stats_sec['max_ts']\n",
    "df_log_on_off_stats['off_mode_ts'] = df_user_logon_stats_sec['mode_ts']\n",
    "df_log_on_off_stats['off_mean_ts'] = df_user_logon_stats_sec['mean_ts']\n",
    "\n",
    "\n",
    "log_stats = df_log_on_off_stats.drop(['User'], axis=1)\n",
    "log_stats_array = np.array(log_stats)  # Changed from np.matrix to np.array\n",
    "print(log_stats_array)\n",
    "\n",
    "\n",
    "#logon/logoff model training\n",
    "forest = IsolationForest(bootstrap=False, contamination= 0.1 , max_features=1.0,\n",
    "        max_samples='auto', n_estimators=100, n_jobs=1, random_state=0,\n",
    "        verbose=0)\n",
    "forest.fit(log_stats_array)\n",
    "\n",
    "log_ascore = forest.decision_function(log_stats_array)\n",
    "log_ascore[:10]\n",
    "print(log_ascore[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "02b570f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating testing data\n",
    "\n",
    "# Group by 'User' and aggregate times for logon\n",
    "df_user_logon_stats = logon_test_df.groupby('User')['Time'].agg(['min', 'max']).reset_index()\n",
    "df_logon_mode = logon_test_df.groupby('User')['Time'].agg(lambda x: x.value_counts().index[0]).reset_index()\n",
    "df_logon_mean = logon_test_df.groupby('User')['Hour'].mean().reset_index()\n",
    "\n",
    "# Convert mean hour to int and then to time\n",
    "df_logon_mean['Hour'] = df_logon_mean['Hour'].astype(int)\n",
    "df_logon_mean['Hour'] = pd.to_datetime(df_logon_mean['Hour'], format='%H').dt.time\n",
    "\n",
    "# Add mode and mean to the logon stats DataFrame\n",
    "df_user_logon_stats['mode'] = df_logon_mode['Time']\n",
    "df_user_logon_stats['mean'] = df_logon_mean['Hour']\n",
    "\n",
    "# Group by 'User' and aggregate times for logoff\n",
    "df_user_logoff_stats = logoff_test_df.groupby('User')['Time'].agg(['min', 'max']).reset_index()\n",
    "df_logoff_mode = logoff_test_df.groupby('User')['Time'].agg(lambda x: x.value_counts().index[0]).reset_index()\n",
    "df_logoff_mean = logoff_test_df.groupby('User')['Hour'].mean().reset_index()\n",
    "\n",
    "# Convert mean hour to int and then to time\n",
    "df_logoff_mean['Hour'] = df_logoff_mean['Hour'].astype(int)\n",
    "df_logoff_mean['Hour'] = pd.to_datetime(df_logoff_mean['Hour'], format='%H').dt.time\n",
    "\n",
    "# Add mode and mean to the logoff stats DataFrame\n",
    "df_user_logoff_stats['mode'] = df_logoff_mode['Time']\n",
    "df_user_logoff_stats['mean'] = df_logoff_mean['Hour']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b44852da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[27314 49482 28461 ... 49482 28461 32400]\n",
      " [28208 29336 28989 ... 29336 28989 25200]\n",
      " [28234 29391 29219 ... 29391 29219 25200]\n",
      " ...\n",
      " [23706 57703 24550 ... 57703 24550 28800]\n",
      " [30020 31173 30716 ... 31173 30716 28800]\n",
      " [32702 33891 33602 ... 33891 33602 32400]]\n"
     ]
    }
   ],
   "source": [
    "df_user_logon_stats_sec = df_user_logon_stats\n",
    "\n",
    "time_columns = ['min', 'max', 'mode', 'mean']\n",
    "for col in time_columns:\n",
    "    df_user_logon_stats_sec[col] = pd.to_datetime(df_user_logon_stats[col], format='%H:%M:%S').dt.time\n",
    "\n",
    "\n",
    "min_ts = [dtt2timestamp(dtt) for dtt in df_user_logon_stats_sec['min']]\n",
    "max_ts = [dtt2timestamp(dtt) for dtt in df_user_logon_stats_sec['max']]\n",
    "mode_ts = [dtt2timestamp(dtt) for dtt in df_user_logon_stats_sec['mode']]\n",
    "mean_ts = [dtt2timestamp(dtt) for dtt in df_user_logon_stats_sec['mean']]\n",
    "\n",
    "df_user_logon_stats_sec['min_ts'] = min_ts\n",
    "df_user_logon_stats_sec['max_ts'] = max_ts\n",
    "df_user_logon_stats_sec['mode_ts'] = mode_ts\n",
    "df_user_logon_stats_sec['mean_ts'] = mean_ts\n",
    "\n",
    "\n",
    "df_user_logon_stats_sec.drop(['min','max','mode','mean'], axis=1)\n",
    "\n",
    "df_user_logoff_stats_sec = df_user_logoff_stats\n",
    "\n",
    "for col in time_columns:\n",
    "    df_user_logoff_stats_sec[col] = pd.to_datetime(df_user_logoff_stats[col], format='%H:%M:%S').dt.time\n",
    "\n",
    "\n",
    "min_ts = [dtt2timestamp(dtt) for dtt in df_user_logoff_stats_sec['min']] \n",
    "max_ts = [dtt2timestamp(dtt) for dtt in df_user_logoff_stats_sec['max']]\n",
    "mode_ts = [dtt2timestamp(dtt) for dtt in df_user_logoff_stats_sec['mode']]\n",
    "mean_ts = [dtt2timestamp(dtt) for dtt in df_user_logoff_stats_sec['mean']]\n",
    "\n",
    "\n",
    "df_user_logoff_stats_sec['min_ts'] = min_ts\n",
    "df_user_logoff_stats_sec['max_ts'] = max_ts\n",
    "df_user_logoff_stats_sec['mode_ts'] = mode_ts\n",
    "df_user_logoff_stats_sec['mean_ts'] = mean_ts\n",
    "\n",
    "\n",
    "df_user_logoff_stats_sec.drop(['min', 'max','mode','mean'], axis=1)\n",
    "\n",
    "\n",
    "df_log_on_off_stats = pd.DataFrame()\n",
    "\n",
    "df_log_on_off_stats['User'] = df_user_logon_stats_sec['User']\n",
    "df_log_on_off_stats['on_min_ts'] = df_user_logon_stats_sec['min_ts']\n",
    "df_log_on_off_stats['on_max_ts'] = df_user_logon_stats_sec['max_ts']\n",
    "df_log_on_off_stats['on_mode_ts'] = df_user_logon_stats_sec['mode_ts']\n",
    "df_log_on_off_stats['on_mean_ts'] = df_user_logon_stats_sec['mean_ts']\n",
    "df_log_on_off_stats['off_min_ts'] = df_user_logon_stats_sec['min_ts']\n",
    "df_log_on_off_stats['off_max_ts'] = df_user_logon_stats_sec['max_ts']\n",
    "df_log_on_off_stats['off_mode_ts'] = df_user_logon_stats_sec['mode_ts']\n",
    "df_log_on_off_stats['off_mean_ts'] = df_user_logon_stats_sec['mean_ts']\n",
    "\n",
    "\n",
    "log_stats = df_log_on_off_stats.drop(['User'], axis=1)\n",
    "log_stats_array = np.array(log_stats)  # Changed from np.matrix to np.array\n",
    "print(log_stats_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fa6e4d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>User</th>\n",
       "      <th>on_min_ts</th>\n",
       "      <th>on_max_ts</th>\n",
       "      <th>on_mode_ts</th>\n",
       "      <th>on_mean_ts</th>\n",
       "      <th>off_min_ts</th>\n",
       "      <th>off_max_ts</th>\n",
       "      <th>off_mode_ts</th>\n",
       "      <th>off_mean_ts</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>DTAA/AAA0371</td>\n",
       "      <td>27314</td>\n",
       "      <td>49482</td>\n",
       "      <td>28461</td>\n",
       "      <td>32400</td>\n",
       "      <td>27314</td>\n",
       "      <td>49482</td>\n",
       "      <td>28461</td>\n",
       "      <td>32400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>DTAA/AAC0344</td>\n",
       "      <td>28208</td>\n",
       "      <td>29336</td>\n",
       "      <td>28989</td>\n",
       "      <td>25200</td>\n",
       "      <td>28208</td>\n",
       "      <td>29336</td>\n",
       "      <td>28989</td>\n",
       "      <td>25200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>DTAA/AAC0599</td>\n",
       "      <td>28234</td>\n",
       "      <td>29391</td>\n",
       "      <td>29219</td>\n",
       "      <td>25200</td>\n",
       "      <td>28234</td>\n",
       "      <td>29391</td>\n",
       "      <td>29219</td>\n",
       "      <td>25200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DTAA/AAH0734</td>\n",
       "      <td>30005</td>\n",
       "      <td>31178</td>\n",
       "      <td>30844</td>\n",
       "      <td>28800</td>\n",
       "      <td>30005</td>\n",
       "      <td>31178</td>\n",
       "      <td>30844</td>\n",
       "      <td>28800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>DTAA/AAK0658</td>\n",
       "      <td>30005</td>\n",
       "      <td>31155</td>\n",
       "      <td>31015</td>\n",
       "      <td>28800</td>\n",
       "      <td>30005</td>\n",
       "      <td>31155</td>\n",
       "      <td>31015</td>\n",
       "      <td>28800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>DTAA/ZGH0528</td>\n",
       "      <td>30906</td>\n",
       "      <td>53306</td>\n",
       "      <td>30924</td>\n",
       "      <td>36000</td>\n",
       "      <td>30906</td>\n",
       "      <td>53306</td>\n",
       "      <td>30924</td>\n",
       "      <td>36000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>DTAA/ZKE0662</td>\n",
       "      <td>26405</td>\n",
       "      <td>48999</td>\n",
       "      <td>27448</td>\n",
       "      <td>32400</td>\n",
       "      <td>26405</td>\n",
       "      <td>48999</td>\n",
       "      <td>27448</td>\n",
       "      <td>32400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>DTAA/ZKH0388</td>\n",
       "      <td>23706</td>\n",
       "      <td>57703</td>\n",
       "      <td>24550</td>\n",
       "      <td>28800</td>\n",
       "      <td>23706</td>\n",
       "      <td>57703</td>\n",
       "      <td>24550</td>\n",
       "      <td>28800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>DTAA/ZKN0548</td>\n",
       "      <td>30020</td>\n",
       "      <td>31173</td>\n",
       "      <td>30716</td>\n",
       "      <td>28800</td>\n",
       "      <td>30020</td>\n",
       "      <td>31173</td>\n",
       "      <td>30716</td>\n",
       "      <td>28800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>DTAA/ZRR0705</td>\n",
       "      <td>32702</td>\n",
       "      <td>33891</td>\n",
       "      <td>33602</td>\n",
       "      <td>32400</td>\n",
       "      <td>32702</td>\n",
       "      <td>33891</td>\n",
       "      <td>33602</td>\n",
       "      <td>32400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             User  on_min_ts  on_max_ts  on_mode_ts  on_mean_ts  off_min_ts  \\\n",
       "0    DTAA/AAA0371      27314      49482       28461       32400       27314   \n",
       "1    DTAA/AAC0344      28208      29336       28989       25200       28208   \n",
       "2    DTAA/AAC0599      28234      29391       29219       25200       28234   \n",
       "3    DTAA/AAH0734      30005      31178       30844       28800       30005   \n",
       "4    DTAA/AAK0658      30005      31155       31015       28800       30005   \n",
       "..            ...        ...        ...         ...         ...         ...   \n",
       "995  DTAA/ZGH0528      30906      53306       30924       36000       30906   \n",
       "996  DTAA/ZKE0662      26405      48999       27448       32400       26405   \n",
       "997  DTAA/ZKH0388      23706      57703       24550       28800       23706   \n",
       "998  DTAA/ZKN0548      30020      31173       30716       28800       30020   \n",
       "999  DTAA/ZRR0705      32702      33891       33602       32400       32702   \n",
       "\n",
       "     off_max_ts  off_mode_ts  off_mean_ts  \n",
       "0         49482        28461        32400  \n",
       "1         29336        28989        25200  \n",
       "2         29391        29219        25200  \n",
       "3         31178        30844        28800  \n",
       "4         31155        31015        28800  \n",
       "..          ...          ...          ...  \n",
       "995       53306        30924        36000  \n",
       "996       48999        27448        32400  \n",
       "997       57703        24550        28800  \n",
       "998       31173        30716        28800  \n",
       "999       33891        33602        32400  \n",
       "\n",
       "[1000 rows x 9 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_log_on_off_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2edae985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    834\n",
      "1    166\n",
      "dtype: int64\n",
      "             User  anomaly_score\n",
      "0    DTAA/AAA0371       0.150322\n",
      "1    DTAA/AAC0344       0.160852\n",
      "2    DTAA/AAC0599       0.145199\n",
      "3    DTAA/AAH0734       0.152475\n",
      "4    DTAA/AAK0658       0.148931\n",
      "..            ...            ...\n",
      "995  DTAA/ZGH0528       0.092362\n",
      "996  DTAA/ZKE0662       0.138314\n",
      "997  DTAA/ZKH0388       0.007970\n",
      "998  DTAA/ZKN0548       0.150096\n",
      "999  DTAA/ZRR0705       0.033557\n",
      "\n",
      "[1000 rows x 2 columns]\n",
      "             User  anomaly_score\n",
      "8    DTAA/ABB0272      -0.075030\n",
      "13   DTAA/ABS0726      -0.070226\n",
      "14   DTAA/ACD0647      -0.058164\n",
      "24   DTAA/AFF0760      -0.055586\n",
      "28   DTAA/AFO0022      -0.002733\n",
      "..            ...            ...\n",
      "961  DTAA/WJG0153      -0.047419\n",
      "967  DTAA/WOT0549      -0.012125\n",
      "970  DTAA/WTA0867      -0.070465\n",
      "977  DTAA/XQW0354      -0.031970\n",
      "993  DTAA/ZCA0652      -0.089163\n",
      "\n",
      "[166 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "log_ascore = forest.decision_function(log_stats_array)\n",
    "log_ascore[:10]\n",
    "\n",
    "# counting the values\n",
    "df22 = pd.Series(forest.predict(log_stats_array))\n",
    "df22 = df22.map({1:0, -1:1})\n",
    "print(df22.value_counts())\n",
    "\n",
    "\n",
    "df_user_log_result = pd.DataFrame()\n",
    "df_user_log_result['User'] = df_user_logoff_stats_sec['User']\n",
    "df_user_log_result['anomaly_score'] = log_ascore\n",
    "print(df_user_log_result)\n",
    "\n",
    "\n",
    "outliers = df_user_log_result.loc[df_user_log_result['anomaly_score'] < 0]\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66bfdf32",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_user_logoff_stats_sec.loc[df_user_log_result['anomaly_score'] < 0].hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5968f9d8c4b840a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T19:19:09.122552Z",
     "start_time": "2024-04-12T19:18:55.222195Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Lists to hold data from each file\n",
    "connect_data = []\n",
    "disconnect_data = []\n",
    "\n",
    "# Iterate over each CSV file in the directory\n",
    "for filename in os.listdir(csv_directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        # Full path to the CSV file\n",
    "        csv_path = os.path.join(csv_directory, filename)\n",
    "\n",
    "        # Read the CSV file into a DataFrame\n",
    "        df = pd.read_csv(csv_path, names=['User', 'Timestamp', 'PC', 'ActivityType', 'Action'])\n",
    "\n",
    "        # Convert Timestamp to datetime\n",
    "        df['Timestamp'] = pd.to_datetime(df['Timestamp'], format='%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "        # Filter DataFrame by action and append to the respective list\n",
    "        connect_data.append(df[df['Action'] == 'Connect'])\n",
    "        disconnect_data.append(df[df['Action'] == 'Disconnect'])\n",
    "\n",
    "# Concatenate all connect and disconnect data into two separate DataFrames\n",
    "connect_df = pd.concat(connect_data)\n",
    "disconnect_df = pd.concat(disconnect_data)\n",
    "\n",
    "# Function to format datetime as H:M:S\n",
    "format_time = lambda x: x.strftime('%H:%M:%S')\n",
    "\n",
    "# Calculate means for connect and disconnect\n",
    "connect_means = connect_df.groupby('User')['Timestamp'].apply(lambda x: format_time(x.mean()))\n",
    "disconnect_means = disconnect_df.groupby('User')['Timestamp'].apply(lambda x: format_time(x.mean()))\n",
    "\n",
    "# Calculate modes for connect and disconnect\n",
    "# We use scipy's mode function which returns the first mode in case of multimodal data\n",
    "connect_modes = connect_df.groupby('User')['Timestamp'].apply(lambda x: format_time(x.dt.time.mode()[0]))\n",
    "disconnect_modes = disconnect_df.groupby('User')['Timestamp'].apply(lambda x: format_time(x.dt.time.mode()[0]))\n",
    "\n",
    "# Convert Series to DataFrame for CSV output\n",
    "connect_means_df = connect_means.reset_index().rename(columns={'Timestamp': 'Connect_mean_time'})\n",
    "connect_modes_df = connect_modes.reset_index().rename(columns={'Timestamp': 'Connect_mode_time'})\n",
    "\n",
    "disconnect_means_df = disconnect_means.reset_index().rename(columns={'Timestamp': 'Disconnect_mean_time'})\n",
    "disconnect_modes_df = disconnect_modes.reset_index().rename(columns={'Timestamp': 'Disconnect_mode_time'})\n",
    "\n",
    "# Create stats DataFrame for connection data\n",
    "df_device_conn_stats = connect_df.groupby('User')['Timestamp'].agg(['min', 'max']).reset_index()\n",
    "df_device_conn_stats['min'] = df_device_conn_stats['min'].dt.time.apply(format_time)\n",
    "df_device_conn_stats['max'] = df_device_conn_stats['max'].dt.time.apply(format_time)\n",
    "\n",
    "# Merge the mean and mode dataframes with the stats DataFrames\n",
    "df_device_conn_stats = pd.merge(df_device_conn_stats, connect_means_df, on='User', how='left')\n",
    "df_device_conn_stats = pd.merge(df_device_conn_stats, connect_modes_df, on='User', how='left')\n",
    "\n",
    "# Rename the columns for consistency and clarity\n",
    "df_device_conn_stats.rename(columns={'min': 'Connect_min_time',\n",
    "                                     'max': 'Connect_max_time'}, inplace=True)\n",
    "\n",
    "# Create stats DataFrame for disconnection data\n",
    "df_device_disconn_stats = disconnect_df.groupby('User')['Timestamp'].agg(['min', 'max']).reset_index()\n",
    "df_device_disconn_stats['min'] = df_device_disconn_stats['min'].dt.time.apply(format_time)\n",
    "df_device_disconn_stats['max'] = df_device_disconn_stats['max'].dt.time.apply(format_time)\n",
    "\n",
    "# Merge the mean and mode dataframes with the stats DataFrames\n",
    "df_device_disconn_stats = pd.merge(df_device_disconn_stats, disconnect_means_df, on='User', how='left')\n",
    "df_device_disconn_stats = pd.merge(df_device_disconn_stats, disconnect_modes_df, on='User', how='left')\n",
    "\n",
    "# Rename the columns for consistency and clarity\n",
    "df_device_disconn_stats.rename(columns={'min': 'Disconnect_min_time',\n",
    "                                        'max': 'Disconnect_max_time'}, inplace=True)\n",
    "\n",
    "# These variables hold the respective statistics for later use in your code:\n",
    "# connect_means_df, connect_modes_df, disconnect_means_df, disconnect_modes_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2357370cf8c26873",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T19:19:09.128842Z",
     "start_time": "2024-04-12T19:19:09.125434Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "connect_means_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d2fdb90030029d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T19:19:09.134086Z",
     "start_time": "2024-04-12T19:19:09.131341Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "connect_modes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e900f5dd178a703b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T19:19:09.142787Z",
     "start_time": "2024-04-12T19:19:09.137484Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disconnect_means_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ef00031d3a4671",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T19:19:09.156925Z",
     "start_time": "2024-04-12T19:19:09.142966Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "disconnect_modes_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create stats DataFrame for connection data\n",
    "df_device_conn_stats = connect_df.groupby('User')['Timestamp'].agg(['min', 'max']).reset_index()\n",
    "df_device_conn_stats['min'] = df_device_conn_stats['min'].dt.time\n",
    "df_device_conn_stats['max'] = df_device_conn_stats['max'].dt.time\n",
    "\n",
    "# Merge the mean and mode dataframes with the stats DataFrames\n",
    "df_device_conn_stats = pd.merge(df_device_conn_stats, connect_means_df, on='User', how='left')\n",
    "df_device_conn_stats = pd.merge(df_device_conn_stats, connect_modes_df, on='User', how='left')\n",
    "\n",
    "# Rename the columns for consistency and clarity\n",
    "df_device_conn_stats.rename(columns={'min': 'Connect_min_time',\n",
    "                                     'max': 'Connect_max_time',\n",
    "                                     'Connect_mean_time': 'Connect_mean_time',\n",
    "                                     'Connect_mode_time': 'Connect_mode_time'}, inplace=True)\n",
    "\n",
    "# Create stats DataFrame for disconnection data\n",
    "df_device_disconn_stats = disconnect_df.groupby('User')['Timestamp'].agg(['min', 'max']).reset_index()\n",
    "df_device_disconn_stats['min'] = df_device_disconn_stats['min'].dt.time\n",
    "df_device_disconn_stats['max'] = df_device_disconn_stats['max'].dt.time\n",
    "\n",
    "# Merge the mean and mode dataframes with the stats DataFrames\n",
    "df_device_disconn_stats = pd.merge(df_device_disconn_stats, disconnect_means_df, on='User', how='left')\n",
    "df_device_disconn_stats = pd.merge(df_device_disconn_stats, disconnect_modes_df, on='User', how='left')\n",
    "\n",
    "# Rename the columns for consistency and clarity\n",
    "df_device_disconn_stats.rename(columns={'min': 'Disconnect_min_time',\n",
    "                                        'max': 'Disconnect_max_time',\n",
    "                                        'Disconnect_mean_time': 'Disconnect_mean_time',\n",
    "                                        'Disconnect_mode_time': 'Disconnect_mode_time'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion of connection times to timestamps in seconds\n",
    "for col in ['Connect_min_time', 'Connect_max_time', 'Connect_mode_time', 'Connect_mean_time']:\n",
    "    # Convert the time strings to datetime.time objects\n",
    "    df_device_conn_stats[col + '_ts'] = pd.to_datetime(df_device_conn_stats[col], format='%H:%M:%S').dt.time\n",
    "    # Apply the dtt2timestamp function to convert to seconds\n",
    "    df_device_conn_stats[col + '_sec'] = df_device_conn_stats[col + '_ts'].apply(dtt2timestamp)\n",
    "\n",
    "# Conversion of disconnection times to timestamps in seconds\n",
    "for col in ['Disconnect_min_time', 'Disconnect_max_time', 'Disconnect_mode_time', 'Disconnect_mean_time']:\n",
    "    # Convert the time strings to datetime.time objects\n",
    "    df_device_disconn_stats[col + '_ts'] = pd.to_datetime(df_device_disconn_stats[col], format='%H:%M:%S').dt.time\n",
    "    # Apply the dtt2timestamp function to convert to seconds\n",
    "    df_device_disconn_stats[col + '_sec'] = df_device_disconn_stats[col + '_ts'].apply(dtt2timestamp)\n",
    "\n",
    "# Merge the connection and disconnection dataframes\n",
    "df_device_full = pd.merge(df_device_conn_stats, df_device_disconn_stats, on='User')\n",
    "\n",
    "# Select columns that end with '_sec' for analysis\n",
    "sec_columns = [col for col in df_device_full.columns if col.endswith('_sec')]\n",
    "device_full_array = df_device_full[sec_columns].values\n",
    "\n",
    "# Set up and train IsolationForest model\n",
    "forest = IsolationForest(bootstrap=False, contamination=0.1, max_features=1.0,\n",
    "                         max_samples='auto', n_estimators=100, n_jobs=1, random_state=None,\n",
    "                         verbose=0)\n",
    "forest.fit(device_full_array)\n",
    "\n",
    "# Save the trained model to a pickle file\n",
    "# with open('../pkl/device_params.pkl', 'wb') as file:\n",
    "#     pickle.dump(forest, file)\n",
    "\n",
    "# Calculate anomaly scores\n",
    "df_device_full['anomaly_score'] = forest.decision_function(device_full_array)\n",
    "\n",
    "# Identifying outliers\n",
    "df_device_full_outliers = df_device_full[df_device_full['anomaly_score'] < 0]\n",
    "print(df_device_full_outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59df449055e44519",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T19:19:09.217390Z",
     "start_time": "2024-04-12T19:19:09.148699Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all = pd.DataFrame()\n",
    "\n",
    "# Read all CSV files and concatenate into a single DataFrame\n",
    "for filename in os.listdir(csv_directory):\n",
    "    if filename.endswith('.csv'):\n",
    "        file_path = os.path.join(csv_directory, filename)\n",
    "        df_temp = pd.read_csv(file_path, header=None)  # Assuming the CSV has no header\n",
    "        df_temp.columns = ['User', 'Timestamp', 'PC', 'Activity', 'Action']  # Assign column names\n",
    "        df_all = pd.concat([df_all, df_temp])\n",
    "\n",
    "# Ensure 'Timestamp' is a datetime object\n",
    "df_all['Timestamp'] = pd.to_datetime(df_all['Timestamp'], format='%m/%d/%Y %H:%M:%S')\n",
    "\n",
    "# Filter out 'Logon' and 'Logoff' activities into separate DataFrames\n",
    "df_logon = df_all[df_all['Action'] == 'Logon']\n",
    "df_logoff = df_all[df_all['Action'] == 'Logoff']\n",
    "\n",
    "# Define a function to perform the aggregation and transformation\n",
    "def aggregate_user_pc_log_activities(df_activity):\n",
    "    # Aggregate data to count the total number of activities per user per PC\n",
    "    df_user_pc = df_activity.groupby(['User', 'PC']).size().reset_index(name='pc_activities_per_user_total')\n",
    "\n",
    "    # Calculate the unique count of PCs per user\n",
    "    df_user_pc['unique_pc_count'] = df_user_pc.groupby('User')['PC'].transform('nunique')\n",
    "\n",
    "    # Keep only the 'User' and 'unique_pc_count' columns and drop duplicates\n",
    "    df_user_pc = df_user_pc[['User', 'unique_pc_count']].drop_duplicates()\n",
    "\n",
    "    return df_user_pc\n",
    "\n",
    "# Apply the function to both logon and logoff DataFrames\n",
    "df_user_pc_logon = aggregate_user_pc_log_activities(df_logon)\n",
    "df_user_pc_logoff = aggregate_user_pc_log_activities(df_logoff)\n",
    "\n",
    "# Optionally, save these dataframes to CSV\n",
    "# df_user_pc_logon.to_csv('user_pc_logon.csv', index=False)\n",
    "# df_user_pc_logoff.to_csv('user_pc_logoff.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b0ee381e4f82a1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T19:19:09.235743Z",
     "start_time": "2024-04-12T19:19:09.157055Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_user_pc_logon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a5af5dcbfc787a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T19:19:09.339994Z",
     "start_time": "2024-04-12T19:19:09.185817Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import IsolationForest\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fba4a7f31b84ac",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T19:19:28.117446Z",
     "start_time": "2024-04-12T19:19:09.479876Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Let's assume df_user_pc_logon and df_user_pc_logoff are obtained from the previous code\n",
    "# We will use df_user_pc_logon for this example\n",
    "\n",
    "# Prepare the data for the Isolation Forest\n",
    "# Reshape the 'unique_pc_count' column to a 2D array\n",
    "pc_counts = np.array(df_user_pc_logon['unique_pc_count']).reshape(-1, 1)\n",
    "\n",
    "# Initialize the Isolation Forest model\n",
    "forest = IsolationForest(bootstrap=False, contamination=0.1, max_features=1.0,\n",
    "                         max_samples='auto', n_estimators=100, n_jobs=1, random_state=None,\n",
    "                         verbose=0)\n",
    "\n",
    "# Fit the model\n",
    "forest.fit(pc_counts)\n",
    "\n",
    "# Serialize the model using pickle\n",
    "with open('user_pc_ct_isolation_forest.pkl', 'wb') as file:\n",
    "    pickle.dump(forest, file)\n",
    "\n",
    "# Get the anomaly scores (the lower, the more abnormal)\n",
    "anomaly_scores = forest.decision_function(pc_counts)\n",
    "\n",
    "# Create a DataFrame to hold the results\n",
    "results = pd.DataFrame()\n",
    "results['user'] = df_user_pc_logon['User']\n",
    "results['unique_pc_count'] = df_user_pc_logon['unique_pc_count']\n",
    "results['anomaly_score'] = anomaly_scores\n",
    "\n",
    "# Identify potential outliers\n",
    "# Outliers are defined as observations with an anomaly score less than 0\n",
    "df_unique_pc_outliers = results.loc[results['anomaly_score'] < 0]\n",
    "\n",
    "# Display the results\n",
    "print(results.head(10))  # Show the first 10 results\n",
    "print(df_unique_pc_outliers)          # Show the outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82113797dfa2fbd8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T19:19:28.125206Z",
     "start_time": "2024-04-12T19:19:28.118042Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "df_user_log_result.head()\n",
    "\n",
    "df_threat_users_log = df_user_log_result.loc[df_user_log_result['anomaly_score'] <= -0.04]\n",
    "print(df_threat_users_log)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e9d79d57479f56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T19:19:28.126716Z",
     "start_time": "2024-04-12T19:19:28.124202Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_log_on_off_stats[df_log_on_off_stats.User.isin(df_threat_users_log.User)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc0bd3dc279fa8a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-04-12T19:19:28.248730Z",
     "start_time": "2024-04-12T19:19:28.131911Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_device_full_outliers.head()\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.gca()\n",
    "\n",
    "df_device_full_outliers.loc[df_device_full_outliers['anomaly_score'] < 0].hist(ax=ax)\n",
    "\n",
    "df_threat_users_device_file = df_device_full_outliers.loc[df_device_full_outliers['anomaly_score'] <= 0]\n",
    "print(df_threat_users_device_file)\n",
    "\n",
    "df_device_full[df_device_full.User.isin(df_threat_users_device_file.User)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,4))\n",
    "ax = fig.gca()\n",
    "\n",
    "df_unique_pc_outliers.loc[df_unique_pc_outliers['anomaly_score'] < 0].hist(ax=ax)\n",
    "\n",
    "df_threat_users_unique_pc = df_unique_pc_outliers.loc[df_unique_pc_outliers['anomaly_score'] <= 0]\n",
    "print(df_threat_users_unique_pc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
